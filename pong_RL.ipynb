{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    '''\n",
    "    Replay memory to store states, actions, rewards, dones for batch sampling\n",
    "    '''\n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        :param capacity: Amount of memory to be stored in the buffer\n",
    "        '''\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, done, next_state):\n",
    "        '''\n",
    "        :param state: current state, atari_wrappers.LazyFrames object\n",
    "        :param action: action\n",
    "        :param reward: reward for the action\n",
    "        :param done: \"done\" flag is True when the episode finished\n",
    "        :param next_state: next state, atari_wrappers.LazyFrames object\n",
    "        '''\n",
    "        experience = (state, action, reward, done, next_state)\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Samples the data from the buffer of a desired size\n",
    "        \n",
    "        :param batch_size: sample batch size\n",
    "        :return: batch of (states, actions, rewards, dones, next states).\n",
    "                 all are numpy arrays. states and next states have shape of \n",
    "                 (batch_size, frames, width, height), where frames = 4.\n",
    "                 actions, rewards and dones have shape of (batch_size,)\n",
    "        '''\n",
    "        if len(self.buffer) < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count())\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "        '''\n",
    "        batch is a random sample from buffer containing experiences(state, action, reward, done and next_state)\n",
    "        '''\n",
    "        state_batch = np.array([np.array(experience[0]) for experience in batch])\n",
    "        action_batch = np.array([np.array(experience[1]) for experience in batch])\n",
    "        reward_batch = np.array([np.array(experience[2]) for experience in batch])\n",
    "        done_batch = np.array([np.array(experience[3]) for experience in batch])\n",
    "        next_state_batch = np.array([np.array(experience[4]) for experience in batch])\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, done_batch, next_state_batch\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''\n",
    "    Deep Q-Network\n",
    "    '''\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size =8, stride=4, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Forward propogation\n",
    "        \n",
    "        :param inputs: images. expected sshape is (batch_size, frames, width, height)\n",
    "        '''\n",
    "#         print(inputs.size())\n",
    "        #Change input size according to the expected input\n",
    "        inputs = inputs.view(inputs.size(0), -1, inputs.size(1), inputs.size(2))\n",
    "        \n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from atari_wrappers import wrap_deepmind\n",
    "import datetime\n",
    "\n",
    "class PongAgent():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor for the agent\n",
    "        '''\n",
    "        self.env = wrap_deepmind(gym.make('PongDeterministic-v4'))\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        \n",
    "        self.dqn = DQN(self.num_actions).to(device)\n",
    "        self.target_dqn = DQN(self.num_actions).to(device)\n",
    "        \n",
    "        summary(self.dqn.to(device), (84, 84, 4))\n",
    "        self.buffer = ReplayMemory(1000000)\n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optim = optim.RMSprop(self.dqn.parameters(), lr = 0.0001)\n",
    "        \n",
    "        self.out_dir = './model'\n",
    "        \n",
    "        if not os.path.exists(self.out_dir):\n",
    "            os.makedirs(self.out_dir)\n",
    "            \n",
    "    def to_var(self, x):\n",
    "        '''\n",
    "        Converts x to Variable\n",
    "        \n",
    "        :param x: torch Tensor\n",
    "        :return: torch Variable\n",
    "        '''\n",
    "        x_var = Variable(x).to(device)\n",
    "        return x_var\n",
    "    \n",
    "    def predict_q_values(self, states):\n",
    "        '''\n",
    "        Compute Q values bypassing states through estimation network\n",
    "        \n",
    "        :param states: states, numpy array, the shape is (batch_size, frames, width, height)\n",
    "        :return: actions, Variable, the shape is (batch_size, num_actions)\n",
    "        '''\n",
    "        \n",
    "        states = self.to_var(torch.from_numpy(states).float())\n",
    "        actions = self.dqn(states)\n",
    "        return actions\n",
    "    \n",
    "    def predict_target_q_values(self, states):\n",
    "        '''\n",
    "        Compute Q values bypassing states through target network\n",
    "        \n",
    "        :param states: states, numpy array, the shape is (batch_size, frames, width, height)\n",
    "        :return: actions, Variable, the shape is (batch_size, num_actions)\n",
    "        '''\n",
    "        states = self.to_var(torch.from_numpy(states).float())\n",
    "        actions = self.target_dqn(states)\n",
    "        return actions\n",
    "    \n",
    "    def select_actions(self, state, epsilon):\n",
    "        '''\n",
    "        Select action according to epsilon greedy policy. We will sometimes use \n",
    "        our model for choosing the action, and sometimes we will just sample one \n",
    "        uniformly.\n",
    "        \n",
    "        :param state: state, atari_wrappers.LazyFrames object - list of 4 frames,\n",
    "                      each is a shape of (1, width, height)\n",
    "        :param epsilon: epsilon for making choice between random and generated by dqn action\n",
    "        \n",
    "        :return: action index\n",
    "        '''\n",
    "        choice  = np.random.choice([0, 1], p=(epsilon, 1-epsilon))\n",
    "        if choice == 0:\n",
    "            return np.random.choice(range(self.num_actions))\n",
    "        else:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            actions = self.predict_q_values(state)\n",
    "            return np.argmax(actions.data.cpu().numpy())\n",
    "        \n",
    "    def update(self, states, targets, actions):\n",
    "        '''\n",
    "        Compute loss and do a backward propogation\n",
    "        \n",
    "        :param states: states, numpy array, the shape is (batch_size, frames, width, height)\n",
    "        :param targets: actions from target network, numpy array the shape is (batch_size)\n",
    "        :param actions: actions, numpy array, the shape is (batch_size)\n",
    "        '''\n",
    "        targets = self.to_var(torch.unsqueeze(torch.from_numpy(targets).float(), -1))\n",
    "        actions = self.to_var(torch.unsqueeze(torch.from_numpy(actions).long(), -1))\n",
    "        \n",
    "        predicted_values = self.predict_q_values(states)\n",
    "        affected_values = torch.gather(predicted_values, 1, actions)\n",
    "        loss = self.mse_loss(affected_values, targets)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def get_epsilon(self, total_steps, max_epsilon_steps, epsilon_start, epsilon_final):\n",
    "        '''\n",
    "        Calculate epsilon value. It cannot be more than epsilon_start and less\n",
    "        than epsilon final. It is decayed with each step\n",
    "        \n",
    "        :param total_steps: total number of step from the training begin\n",
    "        :param max_epsilon_steps: maximum number of epsilon steps\n",
    "        :param epsilon_start: start epsilon value, e.g. 1\n",
    "        :param epsilon_final: final epsilon value, effectively a limit\n",
    "        :return: calculated epsilon value\n",
    "        '''\n",
    "        return max(epsilon_final, epsilon_start - total_steps/max_epsilon_steps)\n",
    "    \n",
    "    def sync_target_network(self):\n",
    "        '''\n",
    "        Copies weights from estimation to target network\n",
    "        '''\n",
    "        primary_params = list(self.dqn.parameters())\n",
    "        target_params = list(self.target_dqn.parameters())\n",
    "        for i in range(len(primary_params)):\n",
    "            target_params[i].data[:] = primary_params[i].data[:]\n",
    "            \n",
    "    def calculate_q_targets(self, next_states, rewards, dones):\n",
    "        '''\n",
    "        Calculates Q-targets (actions from the target network)\n",
    "        \n",
    "        :param next_states: next states, numpy array, shape is (batch_size, frames, width, height)\n",
    "        :param rewards: rewards, numpy array, shape is (batch_size,)\n",
    "        :param dones: dones, numpy array, shape is (batch_size,)\n",
    "        '''\n",
    "        dones_mask = (dones==1)\n",
    "        \n",
    "        predicted_q_target_values = self.predict_target_q_values(next_states)\n",
    "        next_max_q_value = np.max(predicted_q_target_values.cpu().detach().numpy(), axis=1)\n",
    "        next_max_q_value[dones_mask] = 0  #No max Q-value of game is over\n",
    "        q_targets = rewards + self.discount_factor*next_max_q_value\n",
    "        \n",
    "        return q_targets\n",
    "    \n",
    "    def save_final_model(self):\n",
    "        '''\n",
    "        Saves final model to the disk\n",
    "        '''\n",
    "        filename = '{}/final_model.pth'.format(self.out_dir)\n",
    "        torc.save(self.dqn.state_dict(), filename)\n",
    "        \n",
    "    def save_model_during_training(self, episode):\n",
    "        '''\n",
    "        Saves temporary models to the disk during training\n",
    "        \n",
    "        :param episode: episode number\n",
    "        '''\n",
    "        filename = '{}/current_model_{}'.format(self.out_dir, episode)\n",
    "        torch.save(self.dqn.state_dict(), filename)\n",
    "        \n",
    "    def load_model(self, filename):\n",
    "        '''\n",
    "        Loads model from the disk\n",
    "        \n",
    "        :param filename: model filename\n",
    "        '''\n",
    "        self.dqn.load_state_dict(torch.load(filename))\n",
    "        self.sync_target_network()\n",
    "        \n",
    "    def play(self, episodes):\n",
    "        '''\n",
    "        Plays the game and renders it\n",
    "        \n",
    "        :param episodes: number of episodes to play\n",
    "        '''\n",
    "        for i in range(1, episodes+1):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.select_actions(state, 0)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                self.env.render()\n",
    "                \n",
    "    def close_env(self):\n",
    "        '''\n",
    "        Closes the environment. Should be called to clean-up\n",
    "        '''\n",
    "        self.env.close()\n",
    "    \n",
    "    def train(self, replay_buffer_fill_len, batch_size, episodes, stop_reward,\n",
    "              max_epsilon_steps, epsilon_start, epsilon_final, sync_target_net_freq):\n",
    "        '''\n",
    "        Trains the network\n",
    "        \n",
    "        :param replay_buffer_fill_len: how many elements should replay buffer contain\n",
    "                                       before training start\n",
    "        :param batch_size: batch size\n",
    "        :param episodes: how many episodes (max. value) to iterate\n",
    "        :param stop_reward: running reward value to be reached. upon reaching that\n",
    "                            value the training is stoped\n",
    "        :param max_epsilon_steps: maximum number of epsilon steps\n",
    "        :param epsilon_start: start epsilon value, e.g. 1\n",
    "        :param epsilon_final: final epsilon value, effectively a limit\n",
    "        :param sync_target_net_freq: how often to sync estimation and target networks\n",
    "        '''\n",
    "                \n",
    "        start_time = time.time()\n",
    "        print('Start training at: '+ time.asctime(time.localtime(start_time)))\n",
    "\n",
    "        total_steps = 0\n",
    "        running_episode_reward = 0\n",
    "\n",
    "        #populate replay memory\n",
    "        print(\"Populating replay buffer.... \\n\")\n",
    "        state = self.env.reset()\n",
    "        for i in range(replay_buffer_fill_len):\n",
    "            action = self.select_actions(state, 1)    #Choose a random action\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            self.buffer.add(state, action, reward, done, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "\n",
    "        print(\"Replay buffer populated with {} transitions, start training...\\n\".format(self.buffer.count()))\n",
    "\n",
    "        for i in range(1, episodes+1):\n",
    "            #reset the environment\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "\n",
    "            #reset episode reward and length\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "\n",
    "            #play until it's possible\n",
    "            while not done:\n",
    "                #synchronize target network with estimation network in required frequency\n",
    "                if total_steps%sync_target_net_freq == 0:\n",
    "                    print(\"Synchronizing target network...\\n\")\n",
    "                    self.sync_target_network()\n",
    "\n",
    "                #Calculate epsilon and select greedy action\n",
    "                epsilon = self.get_epsilon(total_steps, max_epsilon_steps, epsilon_start, epsilon_final) \n",
    "                action = self.select_actions(state, epsilon)\n",
    "\n",
    "                #execute action in the environment\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.buffer.add(state, action,  reward, done, next_state)\n",
    "\n",
    "                #sample random minibatch of transcations\n",
    "                s_batch, a_batch, r_batch, d_batch, next_s_batch = self.buffer.sample(batch_size)\n",
    "\n",
    "                #estimate Q value usiing the target network\n",
    "                q_targets = self.calculate_q_targets(next_s_batch, r_batch, d_batch)\n",
    "\n",
    "                #update weights in the estimation network\n",
    "                self.update(s_batch, q_targets, a_batch)\n",
    "\n",
    "                #set the state for the next action selection and update the reward and counters\n",
    "                state = next_state\n",
    "                total_steps += 1\n",
    "                episode_length += 1\n",
    "                episode_reward += reward\n",
    "\n",
    "            running_episode_reward = running_episode_reward*0.9 + 0.1*episode_reward\n",
    "\n",
    "            if (i % 10) == 0 or (running_episode_reward > stop_reward):\n",
    "                print('global step: {}'.format(total_steps))\n",
    "                print('episode: {}'.format(i))\n",
    "                print('running reward: {}'.format(round(running_episode_reward, 2)))\n",
    "                print('current epsilon: {}'.format(round(epsilon, 2)))\n",
    "                print('episode_length: {}'.format(episode_length))\n",
    "                print('episode reward: {}'.format(episode_reward))\n",
    "                print('\\n')\n",
    "\n",
    "            if (i % 50) == 0 or (running_episode_reward > stop_reward):\n",
    "                curr_time = time.time()\n",
    "                print('current time: ' + time.asctime(time.localtime(curr_time)))\n",
    "                print('running for: ' + str(datetime.timedelta(seconds=curr_time - start_time)))\n",
    "                print('saving model after {} episodes...'.format(i))\n",
    "                print('\\n')\n",
    "                self.save_model_during_training(i)\n",
    "\n",
    "            if running_episode_reward > stop_reward:\n",
    "                print('stop reward reached!')\n",
    "                print('saving final model...')\n",
    "                print('\\n')\n",
    "                self.save_final_model()\n",
    "                break\n",
    "\n",
    "        print('Finish training at: '+ time.asctime(time.localtime(start_time)))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 20, 20]           8,224\n",
      "            Conv2d-2             [-1, 64, 9, 9]          32,832\n",
      "            Conv2d-3             [-1, 64, 7, 7]          36,928\n",
      "            Linear-4                  [-1, 512]       1,606,144\n",
      "            Linear-5                    [-1, 6]           3,078\n",
      "================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 0.17\n",
      "Params size (MB): 6.44\n",
      "Estimated Total Size (MB): 6.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent = PongAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training at: Fri May  8 00:12:42 2020\n",
      "Populating replay buffer.... \n",
      "\n",
      "Replay buffer populated with 100 transitions, start training...\n",
      "\n",
      "Synchronizing target network...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-1162b57d0729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mepsilon_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mepsilon_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             sync_target_net_freq=10000)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-693ba1fc37b1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, replay_buffer_fill_len, batch_size, episodes, stop_reward, max_epsilon_steps, epsilon_start, epsilon_final, sync_target_net_freq)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                 \u001b[1;31m#sample random minibatch of transcations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 \u001b[0ms_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_s_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[1;31m#estimate Q value usiing the target network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-5cc760fb5c7d>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mreward_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexperience\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mdone_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexperience\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mnext_state_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexperience\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(replay_buffer_fill_len=100, \n",
    "            batch_size=32, \n",
    "            episodes=10**5,\n",
    "            stop_reward=19,\n",
    "            max_epsilon_steps=10**5,\n",
    "            epsilon_start=1.0,\n",
    "            epsilon_final=0.02,\n",
    "            sync_target_net_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_model('./model/current_model_350')\n",
    "agent.play(3)\n",
    "agent.close_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
